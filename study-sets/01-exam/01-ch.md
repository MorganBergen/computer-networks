#  chapter 1 study sets

1 kb = 8,000 b		1 kilobyte = 8,000 bits
1 mb = 8 * 106 b		1 megabyte = 8,000,000 bits	
1 gb = 8 * 109 b		1 gigabyte = 8,000,000,000 bits
1 gb = 1,000 mb		1 gigabyte = 1,000 megabytes 
1 km = 1,000 m 		1 kilometer = 1,000 meters 	
1 second = 1000000 microseconds 
1 second = 1000 milliseconds

transmission_delay = L (bits) / R (bits/sec)
L:  packet length (bits)
R:  link transmission rate (bps)

propagation_delay = d / s
d:  length of physical link (m)
s:  propagation speed ~2 * 108 (m / sec)

total_delay = transmission_delay + propagation_delay

queuing_delay = L * a / R : arrival rate of bits / service rate of bits
a:  average packet arrival rate
L:  packet length (bits)
R:  link bandwidth (bit transmission rate)

queuing_delay = i(L / r)(1 - i) * 1000

packets left in buffer = packets_rounded_down(time/delay) = a - floor(1000/delay)
packet_dropped = packets - buffer_size

per_connection_end_to_end_throughput: min(Rc, Rs, R / 10)

time_elapsed = (rtt * num_packets) + (num_packets * ((100 - percent_not_cached) / 100) * trans_delay)

Usender: utilization - fraction of time sender busy sending
Usender = (L / R) / (RTT + (L / R))
Dtrans = L / R
L : packet length (bits)
R : link transmission rate (bps)

access_link_utilization_no_cache = average_data_rate_to_browsers / access_link_rate
LAN_utilization_no_cache = average_data_rate_to_browsers / LAN_link_rate
end-end_delay_no_cache = internet_delay(aka rtt) + access_link_delay + LAN_delay

access_link_utilization_with_cache = (percentage_of_requests_at_origin_servers * access_link_rate) * average_data_rate_to_browsers

estimated_rtt =  (1 - ) * estimated_rtt +  * sample_rtt
 = 0.125

dev_rtt = (1 - ) * dev_rtt +  * |sample_rtt - estimated_rtt|
 = 0.25

timeout_interval = estimated_rtt + 4 * dev_rtt
 link_utilization = actual_data_rate / max_capacity =
shared_link_utilization = actual_data_rate * total_servers / max_capacityhow to connect end systems to edge routers
residential access networks, institutional access networks (school, company), mobile access networks (wifi, 4g/5g)

how do you connect millions of access isp’s together?
connect each access isp to one global transit isp, customer and provider isp’s have economic agreement.  but if one global isp is viable business, there will be competitors…  who will want to be connected and regional networks may arise to connect access networks to isp’s and content provider networks (e.g. google, microsoft, akamai) may run their own network, to bring services, content close to end users.

does the ip address of a host on which a process runs suffice for identifying the process?
no, many processes can be running on the same host.

how can state be preserved and managed between a user’s browser and the server across multiple interactions?
at protocol endpoints:  this involves maintaining state information directly at the sender (client) and receiver (server) endpoints over multiple transactions.  for instance a server might store session data that is accessed across different requests.
in messages:  this involves embedding state information within http messages themselves.  cookies are a primary mechanism for carrying state information in http messages.

why is there web caching?
web caching is a technique used to store copies of frequently accessed web content in locations closer to the user, such as local servers or within a network.  this practice brings multiple benefits including… reducing response time because the cache is closer to the client.  decreasing traffic on networks specifically institutional access links.  and enables poor content providers to more effectively deliver content.

why don’t we centralize dns?
single point of failure, traffic volume, distant centralized database, and maintenance.  and it doesn’t scale.  

what are the differences between the client-server approach and the peer-to-peer approach?
for a client-server approach servers are always-on host, have permanent ip addresses, and often are in data centers for scaling.  clients contact and communicate with a server and may be intermittently connected, clients may have dynamic ip addresses and do not communicate directly with each other.  examples include http, map, and ftp.
for a peer-peer architecture the servers are not always on, arbitrary end system directly communicate with each other.  peer request services from other peers and provide a service in return to other peers.  this approach has self scalability where new peers bring new service capacity as well as new service demands.  peers are intermittently connected, can change ip addresses and have complex management.  examples include p2p file sharing such as BitTorrent.

what are the difference between tcp and udp?
the tcp service has 
 reliable transport: between sending and receiving processes
flow control:  the sender won’t overwhelm the server
congestion control:  throttle sender when network is overloaded
connection oriented:  setup required between client and server processes
does not provide: timing, minimum throughput guarantee, and security
the udp service has
 unreliable data transfer:  between sending and receiving process
does not provide:  reliability, flow control, congestion control, timing, throughput guarantee, security, or connection set up.

what is http, and what is its primary purpose in the context of the world wide web?
the hyper text transfer protocol is the web’s application layer protocol, it is a protocol for transmitting hypertext requests and information on the internet and is the foundation of data communication for the world wide web.  its primary purpose is communication between a client and server, provides resource retrieval, is a stateless protocol, and operates over tcp.

what is the purpose of a cookie in computer networking?
cookies are small pieces of data stored by a web browser on behalf of a website and maintain some state between transactions.  cookies are used for session management such as user authorization, maintaining state, as well as tracking and analytics.

can you mention some application layer protocols and transport layer protocols that we discussed in class along with their abbreviations?
application layer protocols			transport layer protocols
ftp - file transfer protocol				tcp -  transmission control protocol
smtp - simple mail transfer protocol		udp -  user datagram protocol
https - hyper text transfer protocol secure		tls/ssl - transport layer security / secure sockets layer
pop3 - post office protocol version 3		
imap -  internet message access protocol
dns -  domain name system

why is there a udp?
udp - user datagram protocol exists primarily because it offers a lightweight, connectionless communication option compared to tcp.  it has lower overhead, is ideal for real-time applications, has reduced latency, is simple, and is connectionless and stateless.  it also has no setup/handshake needed, it can function when the network is comprised, and helps with reliability.

what do we really mean when we say rdt senders are in a given state or a receiver is in a given state?
In one state, the sender-side protocol is waiting for data to be passed down from the upper layer. In the other state, the sender protocol is waiting for an ack or a nak packet from the receiver.

in rdt (reliable data transfer) how does it recover from errors?
acknowledgments (acks):  receiver explicitly tells sender to that pick received ok
negative acknowledgements (naks):  receiver explicitly tells sender that pck had errors
sender retransmits pkt on receipt of nak

what happens if ack/nak is corrupted
sender retransmits current pkt if ack/nak is corrupted, the sender adds a sequence number to each pkt, and the receiver discards the duplicate pkt.

  how to set tcp timeout value?
tcp controls how long transmitted data may remain unacknowledged before a connection is forcefully closed.  the time out value is is an interval that is calculated as timeout_interval  = estimated_rtt + 4 * dev_rtt

  how to estimate rtt?
you estimate rtt via sample_rtt, which measures the time from segment transmission until the acknowledgement is received.

  what happens if the network layer delivers data faster than the application layer removes data from socket buffers?
flow control:  the receiver controls sender, so sender won’t overflow receiver’s buffer by transmitting too much, too fast.

  what does it mean for a sender and receiver have lot of shared state?
both the sender and the receiver maintain extensive information about the connection between them.  having a lot of shared states means that both sides need to coordinate and update this information frequently to maintain the connection and ensure data integrity.  information includes sequence numbers, acknowledgements, window sizes, timers, and connection states.  this coordination allows tcp to provide features like error recovery, flow control, congestion control, and reliable data transfer.

  how does tcp establishes the shared state?
tcp establishes a shared state through a process called the three-way handshake.  this process ensure that both sides are synchronized and agree on the initial sequence numbers for reliable communication.  
Client            			   Server
  | SYN (seq = x)  ———————————————-—-> |
  |                                    |
  | <—— SYN-ACK (seq = y, ack = x + 1) |
  |                                    |
  | ACK (seq = x + 1, ack = y + 1) ——> |

during this handshake, the following shared state is established with initial sequence numbers, acknowledgement numbers, and connection parameters.

  how does it teardown a connection when a communication is done?
the tcp teardown process involves sending a fin packet, receiving an ack packet, sending a fin packet from the responder, receiving a ack packet, and entering the time wait state.

  will 2-way handshake always work in network?
a two way handshake is a simpler process compared to a three way handshake, but is not reliable for establishing a tcp connection in networks.

  consider sending a packet from a source host to a destination host over a fixed route.  list the delay components in the end-to-end delay.  which of these delays are constant and which are variable?
when sending a packet from a source host to a destination host over a fixed route in a computer network, the end-to-end delay consist of several components.  these delay components can be categorized into four main parts:  transmission delay, propagation delay, processing delay, and queuing delay.  some of these delays are constant while others are variable
transmission delay:  is the time it takes to push all the bits of the packet into the link or transmission medium.  this delay is constant for a given packet size and link bandwidth.  it is determined by the physical properties of the link.
propagation delay:  the time it takes for a single or packet to travel from the source to the destination.  it is related to the distance between the two hosts and the propagation speed of the medium.  this delay is constant as long as the distance and propagation speed remain unchanged.  however it can vary for different routes or network segments.
processing delay:  the time spent by networking devices (routers, switches, and hosts) to process the packet.   this includes the time spent in routing, error checking, and other processing tasks.  this delay is variable depending on the processing workload of the devices along the route.  it can vary due to changes in device congestion and routing decisions.
queuing delay:  the delay that occurs when a packet is waiting in a queue to be transmitted.  this typically occurs in routers or switches when the outgoing link is busy.  this delay is highly variable an dependent on network congestion.  during periods of high traffic, queuing delays can increase significantly.


  what are the five layers in the internet protocol stack?  what are the principle responsibilities of each of these layers?

application layer:  the application layer is the top most layer and is responsible for providing network services directly to end-users and applications.  it includes various protocols that facilitate communication between software applications.  at this layer, data from an application is prepared for transmission over the network.  it is encapsulated within application specific data structures or messages.  for example, in the case of web browsing, an http request is created by the web browser, encapsulating the user’s request for a web page.

transport layer:  the transport layer is responsible for end-to-end communication between devices.  it ensures data is reliably delivered, in order, and provides error checking and flow control.  the data is received from the application layer is encapsulated into segments.  each segment includes a header containing information such as source and destination port numbers.  examples of protocols operating at this layer include tcp and udp.

network layer:  the network layer focuses on routing packets of data between different networks.  it deals with logical addressing, routing, and forwarding.  the segments from the transport layer are further encapsulated into datagrams.  these datagrams include information like source and destination ip addresses.  routers use this information to determine the best path for datagram delivery.  ip (internet protocol) operates at this layer.

data link layer:  the data link layer is responsible for the reliable transmission of frames between devices on the same network segment.  it ensures data integrity and manages access to the physical media.  the datagrams from the network layer are encapsulated into frames.  frames include control information like mac addresses for source and destination devices.  ethernet is a common data link layer technology.

physical layer:  the physical layer deals with the actual transmission of raw bits over the physical medium, considering electrical, mechanical, and functional characteristics of hardware.  at the physical layer, the frames are converted into electrical signals, optical signals, or radio waves suitable for transmission over the physical medium.  this involves encoding and modulation to convert digital data into a physical signal that can traverse the medium.

encapsulation ensures that data is packaged appropriately at each layer of the protocol stack.  as data flows from the application layer down to the physical layer, each layer adds its own header and possibly a trailer to the data, creating a hierarchical structure of encapsulated information.  when data is received at the destination, the process is reversed, and each layer strips off its respective header and trailer until the original data is extracted and delivered to the receiving application.  this encapsulation and decapsulation process allows for the proper transmission and reception of data across a network while maintaining the necessary control and addressing information at each layer.

  what is http and what is its primary purpose in web communications?  compare and contrast the key features and improvements of http/1, http/2, and http/3.  discuss how each protocol addresses performance, efficiency, security, and other aspects of web communication.
http - hyper text transfer protocol is an application layer protocol for web communication, primarily used to transfer web content between servers and browsers.
http/1 uses a single connection for each request/response, leading to potential latency issues.  http/1 is the oldest and simplest protocol but lacks efficiency and performance compared to http/2 and http/3.
http/2 improves performance with multiplexing and header compression enhancing both speed and efficiency.
http/3 utilizes the quic protocol instead of tcp, excels in high-latency scenarios, enhancing performance and security with further efficiency and improvements in modern network environments.

  explain the concept of persistent http connection and its advantages.  how does it differ from non-persistent http connection.  provide examples of scenarios where each type of connection might be beneficial.
a persistent http connection enables multiple requests and responses to use a single network connection, reducing connection setup overhead.  it minimizes latency by avoiding the need to repeatedly establish connections.  fewer connections save server and network resources.  it is ideal for complex web pages with multiple assets and scripts, streaming services, and interactive applications where keeping a connection alive reduces latency and enhances user experience.

non-persistent http connections open a new connection for each request, leading to higher latency and less efficiency.  non-persistent connections work well for simpler, standalone requests.  non-persistent http is suitable for simple web pages, resource-constrained servers, and security-sensitive transactions, where the overhead of connection reuse may not be necessary.

how does http maintain statelessness in its communication?  what are the advantages and disadvantages of statelessness in http?
http treats each request as a separate event without remembering what happened before, keeping no memory on the server about past interactions.
advantages:  it makes servers easier to design and handle many users.  stateless servers can serve lots of clients efficiently.  less risk of server issues due to memory problems.
disadvantages:  it can be challenging to remember what users did in the past.  sometimes methods like cookies are used, which can slow down requests.  managing state on the client side can be complex for developers, especially in complex applications.

  explain the request and response model in http.  what are the key components of an http request and an http response?
the request response model in http is a way for clients to ask for information or actions from servers and receive a reply
http request components:  methods, uri, http version, headers, and body
http response components:  status code, http version, headers, and body

what is a cookie in the context of web technology?  how is it different from other methods of storing user data on the client side?
a cookie in web technology is a small piece of data sent by a web server and stored in a user’s browser for tracking and personalization.  cookies are limited in size, have expiration dates, and are sent with every request, while other methods like web storage and cache api offer more storage capacity, manual data retrieval, and are better for structured or larger data.

  what are the differences between first party cookies and third party cookies?  provide examples of when each type is used and potential privacy concerns associated with third party cookies.
first party cookies are used by websites you’re visiting
example:  remembering your login information
lower privacy concerns because they’re site specific.
third party cookies:  set by other domains, often for tracking 
example:  tracking your online behavior for ads.
higher privacy concerns as they can track you across websites without explicit consent.

  what is web caching, and why is it essential in the context of web performance and user experience?
web caching is like temporarily storing web content to make websites load faster.  it is important because it speeds up page loading, reduces server work, saves bandwidth, and helps website handle lots of users without crashing.  this makes website more reliable and cost effective.

  explain the relationship between browser caching and conditional GET requests in web development.  how do conditional GET requests improve caching efficiency?
in web development, browser caching stores web objects on your computer to make website load faster.  conditional GET requests help by checking if these stored things are still okay to use.  this saves time and data because you only downloaded what’s changed, making websites quicker and using less internet.

  what is dns, and what is its primary function in computer networking and the internet?
dns (domain name system) is like the internet’s address book.  its main job is it change human-readable web addresses (like example.com) into computer-friendly numbers (like ip addresses) so that computers can find and communicate with each other on the internet.  dns also helps balance traffic, manage email, and ensure internet reliability.

  explain the hierarchical structure of the dns naming system, including the roles of top-level domains (olds), second-level domains, and sub domains.
the dns is a distributed hierarchical database.  the root of this tree, we have the root dns servers.  the next layer we have the dns servers that are responsible for all of the .com, .edu, .net domain names and these are known as the top level domain or tld servers.  then we have the authoritative name servers.  these are the servers that have the ultimate responsibility, for resolving names within their domain.  for example wichita.edu and nyu.edu.

  what are authoritative dns servers, and how do they differ from recursive dns servers?  how do authoritative dns servers help resolve domain names?
authoritative dns servers hold official records for specific websites.  recursive dns servers help your computer find the right records by asking authoritative servers and caching answers.  when you want to go to a website, your computer asks a recursive server, which checks its memory or asks others.  it keeps asking until it gets the real answer from authoritative servers.  this way, you can find websites using their names.

  what is ttl (time-to-live) value in dns records, and why is it important?  how does it affect caching and dns propagation?
time-to-live (ttl) is a numerical value used for data validity and expiration.  ttl tells how long a dns record is valid, measured in seconds.  it’s vital for managing dns caching and how quickly changes to dns records spread across the internet.
short ttl’s mean quicker updates but more server load; longer tt’s reduce load but slow updates.

  what are two technical challenges associated with streaming video that we discussed in class, and could you please explain them briefly?
bandwidth limitations:  one of the primary challenges in streaming video is the limitation of available bandwidth.  bandwidth determines how much data can be transmitted over the internet in a given amount of time.  if a user’s internet connection lacks sufficient bandwidth to support the video’s required bitrates, it can result in buffering, lower video quality, or even playback interruptions.
latency and real-time streaming:  latency or the delay between when a video is encoded and when it is displayed on the viewer’s screen, is a critical challenge, especially for real-time streaming applications like live broadcasts or video conferencing.  high latency can cause synchronization issues and affect user engagement.

  what is the role of a buffer in streaming video playback?  how does it contribute to a smooth and uninterrupted viewing experience?
the role of a buffer in streaming video playback is crucial for ensuring a smooth an uninterrupted viewing experience.  the buffer in streaming video playback stores a portion of the video ahead of what’s being watched.  it compensates for network issues, ensuring smooth playback by preloading data and adapting to variable network speeds.

  what is dynamic adaptive streaming over http (dash), and how does it work to deliver video content to viewers?  how does it differ from traditional streaming methods?
dynamic adaptive streaming over http (dash) is streaming protocol that divides video content into small segments, encodes them at multiple quality levels, and uses adaptive streaming to adjust video quality in real-time based on network conditions.  dash relies on standard http for content delivery.  
traditional streaming methods often rely on a fixed bitrate, which can lead to buffering or low-quality playback on variable networks and may involve proprietary or non-standard protocols.
dash allows for efficient use of bandwidth by only fetching the necessary segments, its standardized, efficient, and flexible approach distinguishes it from traditional streaming methods.

  content distribution networks (cdns) typically adopt one of two different server placement philosophies.  name and briefly describe them.
content distribution networks (cdns) can adopt one of two server placement philosophies.
server-centric cdn:  content is distributed from centralized servers located strategically worldwide.  these servers reduce latency and efficiently handle high traffic, making them ideal for large files.
peer-to-peer cdn:  content is shared among decentralized end-user devices.  peers exchange content, reducing server load.  p2p cdns are cost-effective and resilient, often used for live streaming and large file distribution.
